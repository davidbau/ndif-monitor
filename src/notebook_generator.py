"""Generate per-model Colab notebooks for reproducibility.

Creates model-specific notebooks that can be loaded directly in Colab with:
- pip install nnsight
- Hardcoded model name
- Colab secrets integration for auth tokens
"""

import json
import os
from pathlib import Path
from typing import Dict, List, Optional, Any


def make_cell(cell_type: str, source: List[str], execution_count: Optional[int] = None) -> Dict[str, Any]:
    """Create a notebook cell."""
    cell = {
        "cell_type": cell_type,
        "metadata": {},
        "source": source,
    }
    if cell_type == "code":
        cell["execution_count"] = execution_count
        cell["outputs"] = []
    return cell


def generate_colab_notebook(
    scenario: str,
    model_name: str,
    template_path: Optional[Path] = None,
) -> Dict[str, Any]:
    """Generate a Colab-ready notebook for a specific model and scenario.

    Args:
        scenario: Test scenario name (e.g., "basic_trace")
        model_name: Full model name (e.g., "meta-llama/Llama-3.1-8B")
        template_path: Optional path to template notebook (for reference)

    Returns:
        Notebook as dict (can be saved as .ipynb)
    """
    # Sanitize model name for display
    short_name = model_name.split("/")[-1]

    cells = []

    # Header markdown
    cells.append(make_cell("markdown", [
        f"# NDIF Monitor - {scenario.replace('_', ' ').title()}\n",
        "\n",
        f"**Model:** `{model_name}`\n",
        "\n",
        "This notebook tests nnsight + NDIF functionality. ",
        "Generated by [NDIF Monitor](https://github.com/davidbau/ndif-monitor).\n",
        "\n",
        "## Setup\n",
        "\n",
        "Configure your API keys in Colab Secrets (ðŸ”‘ icon in left sidebar):\n",
        "- `NDIF_API_KEY`: Your NDIF API key from [nnsight.net](https://nnsight.net)\n",
        "- `HF_TOKEN`: Your HuggingFace token (for gated models)\n",
    ]))

    # Install dependencies and configure auth
    cells.append(make_cell("code", [
        "# Install dependencies\n",
        "!pip install -q nnsight torch\n",
        "\n",
        "# Load API keys from Colab secrets into environment\n",
        "# nnsight automatically picks up NDIF_API_KEY from env\n",
        "import os\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    for key in ['NDIF_API_KEY', 'HF_TOKEN']:\n",
        "        try:\n",
        "            os.environ[key] = userdata.get(key)\n",
        "        except:\n",
        "            pass\n",
        "except ImportError:\n",
        "    pass  # Not in Colab, use existing env vars\n",
    ]))

    # Model loading - hardcoded model name
    cells.append(make_cell("code", [
        "# Load model\n",
        "from nnsight import LanguageModel\n",
        "import time\n",
        "\n",
        f"MODEL_NAME = '{model_name}'\n",
        "print(f'Loading {MODEL_NAME}...')\n",
        "\n",
        "start = time.time()\n",
        "model = LanguageModel(MODEL_NAME, device_map='auto')\n",
        "load_time = time.time() - start\n",
        "print(f'Model loaded in {load_time:.1f}s')\n",
    ]))

    # Scenario-specific test code
    if scenario == "basic_trace":
        cells.append(make_cell("markdown", [
            "## Basic Trace Test\n",
            "\n",
            "Tests `model.trace()` functionality with hidden state extraction.\n",
        ]))
        cells.append(make_cell("code", _generate_trace_code(model_name)))
        cells.append(make_cell("code", _generate_validation_code("basic_trace")))

    elif scenario == "generation":
        cells.append(make_cell("markdown", [
            "## Generation Test\n",
            "\n",
            "Tests `model.generate()` functionality.\n",
        ]))
        cells.append(make_cell("code", _generate_generation_code(model_name)))
        cells.append(make_cell("code", _generate_validation_code("generation")))

    elif scenario == "hidden_states":
        cells.append(make_cell("markdown", [
            "## Hidden States Extraction\n",
            "\n",
            "Tests extracting hidden states from all layers.\n",
        ]))
        cells.append(make_cell("code", _generate_hidden_states_code(model_name)))
        cells.append(make_cell("code", _generate_validation_code("hidden_states")))

    # Success cell
    scenario_display = scenario.upper().replace("_", " ")
    cells.append(make_cell("code", [
        "print('\\n' + '=' * 50)\n",
        f"print('{scenario_display} TEST PASSED âœ“')\n",
        "print('=' * 50)\n",
    ]))

    return {
        "nbformat": 4,
        "nbformat_minor": 0,
        "metadata": {
            "colab": {
                "name": f"NDIF_{scenario}_{short_name}.ipynb",
                "provenance": [],
            },
            "kernelspec": {
                "name": "python3",
                "display_name": "Python 3"
            },
            "language_info": {
                "name": "python"
            },
            "accelerator": "GPU"
        },
        "cells": cells
    }


def _get_layer_accessor(model_name: str) -> str:
    """Get the correct layer accessor for a model architecture."""
    model_lower = model_name.lower()
    if 'gpt-j' in model_lower or 'gpt2' in model_lower:
        return "model.transformer.h"
    elif 'gpt-neo' in model_lower or 'pythia' in model_lower:
        return "model.gpt_neox.layers"
    else:
        # Llama, Mistral, Qwen, OLMo, Gemma, etc.
        return "model.model.layers"


def _generate_trace_code(model_name: str) -> List[str]:
    """Generate trace test code for a specific model."""
    layer_accessor = _get_layer_accessor(model_name)
    return [
        "# Run basic trace\n",
        "prompt = 'The quick brown fox jumps over the lazy dog'\n",
        "print(f\"Running trace on: '{prompt}'\")\n",
        "\n",
        "start = time.time()\n",
        "with model.trace(prompt, remote=True):\n",
        f"    hidden = {layer_accessor}[0].output[0].save()\n",
        "\n",
        "trace_time = time.time() - start\n",
        "print(f'Trace completed in {trace_time:.1f}s')\n",
        "print(f'Hidden state shape: {hidden.shape}')\n",
    ]


def _generate_generation_code(model_name: str) -> List[str]:
    """Generate text generation test code."""
    return [
        "# Run generation\n",
        "prompt = 'Once upon a time, there was a'\n",
        "print(f\"Generating from: '{prompt}'\")\n",
        "\n",
        "start = time.time()\n",
        "with model.generate(prompt, max_new_tokens=20, remote=True):\n",
        "    output_ids = model.generator.output.save()\n",
        "\n",
        "gen_time = time.time() - start\n",
        "\n",
        "# Decode output\n",
        "generated_text = model.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(f'Generation completed in {gen_time:.1f}s')\n",
        "print(f'Generated: {generated_text}')\n",
    ]


def _generate_hidden_states_code(model_name: str) -> List[str]:
    """Generate hidden states extraction code.

    Uses list.save() - nnsight 0.5 adds .save() method to built-in types.
    """
    layer_accessor = _get_layer_accessor(model_name)
    return [
        "# Extract hidden states from all layers\n",
        "prompt = 'Hello world'\n",
        "print(f\"Extracting hidden states from: '{prompt}'\")\n",
        "\n",
        f"layers = {layer_accessor}\n",
        "num_layers = len(layers)\n",
        "print(f'Model has {num_layers} layers')\n",
        "\n",
        "start = time.time()\n",
        "with model.trace(prompt, remote=True):\n",
        "    # Collect layer outputs and save as a list\n",
        "    states = [layer.output[0] for layer in layers]\n",
        "    states.save()  # nnsight adds .save() to lists\n",
        "\n",
        "extract_time = time.time() - start\n",
        "print(f'Extraction completed in {extract_time:.1f}s')\n",
        "\n",
        "print(f'\\nExtracted {len(states)} layer states:')\n",
        "for i, state in enumerate(states[:5]):\n",
        "    print(f'  Layer {i}: {state.shape}')\n",
        "if len(states) > 5:\n",
        "    print(f'  ... and {len(states) - 5} more layers')\n",
    ]


def _generate_validation_code(scenario: str) -> List[str]:
    """Generate validation code for a scenario."""
    if scenario == "basic_trace":
        return [
            "# Validate results\n",
            "import torch\n",
            "\n",
            "if 'hidden' not in dir():\n",
            "    raise RuntimeError('Trace was interrupted - hidden state not captured. Try running again.')\n",
            "\n",
            "# Verify shape is reasonable\n",
            "assert len(hidden.shape) >= 2, f'Expected at least 2D tensor, got {hidden.shape}'\n",
            "assert hidden.shape[-1] > 0, 'Hidden dimension should be positive'\n",
            "\n",
            "# Check for NaN/Inf\n",
            "assert not torch.isnan(hidden).any(), 'Hidden state contains NaN values'\n",
            "assert not torch.isinf(hidden).any(), 'Hidden state contains Inf values'\n",
            "\n",
            "print('Validation passed!')\n",
        ]
    elif scenario == "generation":
        return [
            "# Validate generation\n",
            "if 'generated_text' not in dir():\n",
            "    raise RuntimeError('Generation was interrupted - output not captured. Try running again.')\n",
            "\n",
            "assert len(generated_text) > len(prompt), 'No text was generated'\n",
            "assert generated_text.startswith(prompt[:20]), 'Generated text does not start with prompt'\n",
            "\n",
            "print('Validation passed!')\n",
        ]
    elif scenario == "hidden_states":
        return [
            "# Validate hidden states\n",
            "import torch\n",
            "\n",
            "assert len(states) == num_layers, f'Expected {num_layers} states, got {len(states)}'\n",
            "\n",
            "for i, state in enumerate(states):\n",
            "    assert not torch.isnan(state).any(), f'Layer {i} contains NaN'\n",
            "    assert not torch.isinf(state).any(), f'Layer {i} contains Inf'\n",
            "\n",
            "print('Validation passed!')\n",
        ]
    return []


def save_notebook(notebook: Dict[str, Any], path: Path) -> None:
    """Save notebook to file."""
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, "w") as f:
        json.dump(notebook, f, indent=1)


def generate_colab_notebooks_for_model(
    model_name: str,
    output_dir: Path,
    scenarios: List[str] = None,
) -> List[Path]:
    """Generate all Colab notebooks for a model.

    Args:
        model_name: Full model name
        output_dir: Directory to save notebooks (e.g., notebooks/colab/)
        scenarios: List of scenarios to generate (default: all)

    Returns:
        List of paths to generated notebooks
    """
    if scenarios is None:
        scenarios = ["basic_trace", "generation", "hidden_states"]

    # Create model-specific subdirectory
    model_filename = model_name.replace("/", "--")
    model_dir = output_dir / model_filename

    paths = []
    for scenario in scenarios:
        notebook = generate_colab_notebook(scenario, model_name)
        notebook_path = model_dir / f"{scenario}.ipynb"
        save_notebook(notebook, notebook_path)
        paths.append(notebook_path)

    return paths


def generate_all_colab_notebooks(
    models: List[str],
    output_dir: Path,
    scenarios: List[str] = None,
) -> Dict[str, List[Path]]:
    """Generate Colab notebooks for all models.

    Args:
        models: List of model names
        output_dir: Base output directory
        scenarios: Scenarios to generate

    Returns:
        Dict mapping model names to list of notebook paths
    """
    results = {}
    for model in models:
        paths = generate_colab_notebooks_for_model(model, output_dir, scenarios)
        results[model] = paths
        print(f"  Generated {len(paths)} notebooks for {model}")
    return results
