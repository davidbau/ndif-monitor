{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden States Test\n",
    "\n",
    "Tests extracting hidden states from multiple layers via NDIF.\n",
    "\n",
    "**Environment Variables:**\n",
    "- `MODEL_NAME`: Model to test\n",
    "- `NDIF_API`: NDIF API key\n",
    "- `HF_TOKEN`: HuggingFace token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "MODEL_NAME = os.environ.get(\"MODEL_NAME\", \"openai-community/gpt2\")\n",
    "print(f\"Testing model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure NDIF\n",
    "from nnsight import CONFIG\n",
    "\n",
    "NDIF_API = os.environ.get(\"NDIF_API\")\n",
    "if NDIF_API:\n",
    "    CONFIG.set_default_api_key(NDIF_API)\n",
    "    print(\"NDIF API key configured\")\n",
    "\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "if HF_TOKEN:\n",
    "    os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
    "    print(\"HF_TOKEN configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "from nnsight import LanguageModel\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "start = time.time()\n",
    "model = LanguageModel(MODEL_NAME, device_map=\"auto\")\n",
    "load_time = time.time() - start\n",
    "print(f\"Model loaded in {load_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of layers from model config\n",
    "def get_num_layers(config):\n",
    "    \"\"\"Get number of layers from model config.\"\"\"\n",
    "    if hasattr(config, 'num_hidden_layers'):\n",
    "        return config.num_hidden_layers\n",
    "    elif hasattr(config, 'n_layer'):\n",
    "        return config.n_layer\n",
    "    elif hasattr(config, 'n_layers'):\n",
    "        return config.n_layers\n",
    "    else:\n",
    "        raise ValueError(\"Cannot determine number of layers\")\n",
    "\n",
    "n_layers = get_num_layers(model.config)\n",
    "print(f\"Model has {n_layers} layers\")\n",
    "\n",
    "# Select representative layer indices to test (first, quarter, middle, three-quarter, last)\n",
    "test_layer_indices = sorted(set([\n",
    "    0,\n",
    "    n_layers // 4,\n",
    "    n_layers // 2,\n",
    "    3 * n_layers // 4,\n",
    "    n_layers - 1\n",
    "]))\n",
    "print(f\"Testing layers: {test_layer_indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract hidden states from selected layers\n",
    "# NOTE: We can't use list comprehensions with variables inside model.trace()\n",
    "# Instead we extract specific hardcoded layer indices and collect results\n",
    "\n",
    "prompt = \"Hello world\"\n",
    "print(f\"Running trace on: '{prompt}'\")\n",
    "print(f\"Extracting hidden states from {len(test_layer_indices)} layers...\")\n",
    "\n",
    "# Detect architecture from model name\n",
    "model_lower = MODEL_NAME.lower()\n",
    "\n",
    "start = time.time()\n",
    "if 'gpt-j' in model_lower or 'gpt2' in model_lower:\n",
    "    # GPT-2, GPT-J: transformer.h\n",
    "    # Use explicit indices since we can't use variables inside trace\n",
    "    i0, i1, i2, i3, i4 = test_layer_indices[0], test_layer_indices[1], test_layer_indices[2], test_layer_indices[min(3, len(test_layer_indices)-1)], test_layer_indices[-1]\n",
    "    with model.trace(prompt, remote=True):\n",
    "        h0 = model.transformer.h[i0].output[0].save()\n",
    "        h1 = model.transformer.h[i1].output[0].save()\n",
    "        h2 = model.transformer.h[i2].output[0].save()\n",
    "        h3 = model.transformer.h[i3].output[0].save()\n",
    "        h4 = model.transformer.h[i4].output[0].save()\n",
    "    hidden_states = [h0, h1, h2, h3, h4]\n",
    "elif 'gpt-neo' in model_lower or 'pythia' in model_lower:\n",
    "    # GPT-NeoX, Pythia: gpt_neox.layers\n",
    "    i0, i1, i2, i3, i4 = test_layer_indices[0], test_layer_indices[1], test_layer_indices[2], test_layer_indices[min(3, len(test_layer_indices)-1)], test_layer_indices[-1]\n",
    "    with model.trace(prompt, remote=True):\n",
    "        h0 = model.gpt_neox.layers[i0].output[0].save()\n",
    "        h1 = model.gpt_neox.layers[i1].output[0].save()\n",
    "        h2 = model.gpt_neox.layers[i2].output[0].save()\n",
    "        h3 = model.gpt_neox.layers[i3].output[0].save()\n",
    "        h4 = model.gpt_neox.layers[i4].output[0].save()\n",
    "    hidden_states = [h0, h1, h2, h3, h4]\n",
    "else:\n",
    "    # Llama, Mistral, Qwen, OLMo, etc.: model.layers\n",
    "    i0, i1, i2, i3, i4 = test_layer_indices[0], test_layer_indices[1], test_layer_indices[2], test_layer_indices[min(3, len(test_layer_indices)-1)], test_layer_indices[-1]\n",
    "    with model.trace(prompt, remote=True):\n",
    "        h0 = model.model.layers[i0].output[0].save()\n",
    "        h1 = model.model.layers[i1].output[0].save()\n",
    "        h2 = model.model.layers[i2].output[0].save()\n",
    "        h3 = model.model.layers[i3].output[0].save()\n",
    "        h4 = model.model.layers[i4].output[0].save()\n",
    "    hidden_states = [h0, h1, h2, h3, h4]\n",
    "\n",
    "trace_time = time.time() - start\n",
    "print(f\"Extraction completed in {trace_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate all hidden states\n",
    "import torch\n",
    "\n",
    "print(f\"\\nValidating {len(hidden_states)} layer outputs...\")\n",
    "\n",
    "for idx, (layer_idx, hidden) in enumerate(zip(test_layer_indices, hidden_states)):\n",
    "    # Check shape\n",
    "    assert len(hidden.shape) >= 2, f\"Layer {layer_idx}: Expected at least 2D tensor\"\n",
    "    \n",
    "    # Check for NaN/Inf\n",
    "    assert not torch.isnan(hidden).any(), f\"Layer {layer_idx}: Contains NaN\"\n",
    "    assert not torch.isinf(hidden).any(), f\"Layer {layer_idx}: Contains Inf\"\n",
    "    \n",
    "    # Check reasonable values\n",
    "    max_val = hidden.abs().max().item()\n",
    "    assert max_val < 10000, f\"Layer {layer_idx}: Values too large ({max_val})\"\n",
    "    \n",
    "    print(f\"  Layer {layer_idx}: shape={hidden.shape}, max={max_val:.2f}\")\n",
    "\n",
    "# Check shapes are consistent\n",
    "shapes = [h.shape for h in hidden_states]\n",
    "hidden_dim = shapes[0][-1]\n",
    "for layer_idx, shape in zip(test_layer_indices, shapes):\n",
    "    assert shape[-1] == hidden_dim, f\"Layer {layer_idx}: Inconsistent hidden dim\"\n",
    "\n",
    "print(f\"\\nAll {len(test_layer_indices)} tested layers validated!\")\n",
    "print(f\"Hidden dimension: {hidden_dim}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"HIDDEN STATES \" + \"TEST PASSED\")\n",
    "print(\"=\" * 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
