{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "NDIF_generation_Llama-3.1-405B.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NDIF Monitor - Generation\n",
    "\n",
    "**Model:** `meta-llama/Llama-3.1-405B`\n",
    "\n",
    "This notebook tests nnsight + NDIF functionality. ",
    "Generated by [NDIF Monitor](https://github.com/davidbau/ndif-monitor).\n",
    "\n",
    "## Setup\n",
    "\n",
    "Configure your API keys in Colab Secrets (\ud83d\udd11 icon in left sidebar):\n",
    "- `NDIF_API_KEY`: Your NDIF API key from [nnsight.net](https://nnsight.net)\n",
    "- `HF_TOKEN`: Your HuggingFace token (for gated models)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Install dependencies\n",
    "!pip install -q nnsight torch\n",
    "\n",
    "# Load API keys from Colab secrets into environment\n",
    "# nnsight automatically picks up NDIF_API_KEY from env\n",
    "import os\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    for key in ['NDIF_API_KEY', 'HF_TOKEN']:\n",
    "        try:\n",
    "            os.environ[key] = userdata.get(key)\n",
    "        except:\n",
    "            pass\n",
    "except ImportError:\n",
    "    pass  # Not in Colab, use existing env vars\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load model\n",
    "from nnsight import LanguageModel\n",
    "import time\n",
    "\n",
    "MODEL_NAME = 'meta-llama/Llama-3.1-405B'\n",
    "print(f'Loading {MODEL_NAME}...')\n",
    "\n",
    "start = time.time()\n",
    "model = LanguageModel(MODEL_NAME, device_map='auto')\n",
    "load_time = time.time() - start\n",
    "print(f'Model loaded in {load_time:.1f}s')\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation Test\n",
    "\n",
    "Tests `model.generate()` functionality.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Run generation\n",
    "prompt = 'Once upon a time, there was a'\n",
    "print(f\"Generating from: '{prompt}'\")\n",
    "\n",
    "start = time.time()\n",
    "with model.generate(prompt, max_new_tokens=20, remote=True):\n",
    "    output_ids = model.generator.output.save()\n",
    "\n",
    "gen_time = time.time() - start\n",
    "\n",
    "# Decode output\n",
    "generated_text = model.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(f'Generation completed in {gen_time:.1f}s')\n",
    "print(f'Generated: {generated_text}')\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Validate generation\n",
    "if 'generated_text' not in dir():\n",
    "    raise RuntimeError('Generation was interrupted - output not captured. Try running again.')\n",
    "\n",
    "assert len(generated_text) > len(prompt), 'No text was generated'\n",
    "assert generated_text.startswith(prompt[:20]), 'Generated text does not start with prompt'\n",
    "\n",
    "print('Validation ' + 'passed!')\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print('\\n' + '=' * 50)\n",
    "print('GENERATION TEST ' + 'PASSED \u2713')\n",
    "print('=' * 50)\n"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}